================================================
File: Dockerfile
================================================
# backend/Dockerfile
FROM python:3.10-slim

RUN apt-get update && apt-get install -y \
    redis-server \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app
# Copy the entire backend directory
COPY . .
RUN pip install --no-cache-dir -r requirements.txt
# Expose the port the app runs on
EXPOSE 80

CMD redis-server --daemonize yes && uvicorn app.main:app --host 0.0.0.0 --port 80


================================================
File: requirements.txt
================================================
bcrypt==4.0.1
boto3==1.35.20
fastapi==0.114.0
openai==1.50.2
uvicorn==0.30.6
PyJWT==2.9.0
passlib==1.7.4
pydantic==2.9.2
pydantic[email]==2.9.2
python-multipart==0.0.9
redis==5.1.1
slowapi==0.1.9

================================================
File: start.sh
================================================
#!/bin/bash

# Start Redis in the background
redis-server --daemonize yes

# Start the FastAPI app
uvicorn app.main:app --host 0.0.0.0 --port 80


================================================
File: task-definition.json
================================================
{
    "taskDefinitionArn": "arn:aws:ecs:us-east-1:521896913840:task-definition/medicare:3",
    "containerDefinitions": [
        {
            "name": "medicare-api",
            "image": "521896913840.dkr.ecr.us-east-1.amazonaws.com/medicare/backend:824857ac0f4fa6ac02eb7f3c8669be356d9be6b9",
            "cpu": 0,
            "portMappings": [
                {
                    "name": "medicare-api-80-tcp",
                    "containerPort": 80,
                    "hostPort": 80,
                    "protocol": "tcp",
                    "appProtocol": "http"
                }
            ],
            "essential": true,
            "environment": [

            ],
            "mountPoints": [],
            "volumesFrom": [],
            "systemControls": []
        }
    ],
    "family": "medicare",
    "executionRoleArn": "arn:aws:iam::521896913840:role/ecsTaskExecutionRole",
    "networkMode": "awsvpc",
    "revision": 3,
    "volumes": [],
    "status": "ACTIVE",
    "requiresAttributes": [
        {
            "name": "com.amazonaws.ecs.capability.ecr-auth"
        },
        {
            "name": "ecs.capability.execution-role-ecr-pull"
        },
        {
            "name": "com.amazonaws.ecs.capability.docker-remote-api.1.18"
        },
        {
            "name": "ecs.capability.task-eni"
        }
    ],
    "placementConstraints": [],
    "compatibilities": [
        "EC2",
        "FARGATE"
    ],
    "requiresCompatibilities": [
        "FARGATE"
    ],
    "cpu": "1024",
    "memory": "3072",
    "runtimePlatform": {
        "cpuArchitecture": "X86_64",
        "operatingSystemFamily": "LINUX"
    },
    "registeredAt": "2024-10-03T20:40:36.245Z",
    "registeredBy": "arn:aws:sts::521896913840:assumed-role/AWSReservedSSO_medicare-s3_65b56463e802a56c/sahiruwrcg",
    "tags": []
}

================================================
File: .dockerignore
================================================
# Exclude the virtual environment directory
env/
venv/
__pycache__/
*.pyc
*.pyo
*.pyd
*.db
*.log
*.sqlite3


================================================
File: app/auth.py
================================================
from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.security import OAuth2PasswordBearer
from pydantic import BaseModel
import boto3
from botocore.exceptions import ClientError
from typing import Optional

# Define your FastAPI router
auth_router = APIRouter()

# AWS Cognito configurations
AWS_REGION = "us-east-1"
USER_POOL_ID = "us-east-1_BLvT00Vuc"
CLIENT_ID = "4fbts6s8gikqt541msvqo13n2d"

# Initialize the Cognito client
cognito_client = boto3.client('cognito-idp', region_name=AWS_REGION)

# OAuth2 scheme for token-based authentication
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

# Pydantic models for data validation
class Token(BaseModel):
    access_token: Optional[str]  # Change to Optional
    token_type: Optional[str]     # Change to Optional
    challenge_name: Optional[str] = None  # Include challenge name
    session: Optional[str] = None  # Include session for password reset

@auth_router.post("/token", response_model=Token)
async def login_for_access_token(username: str = Query(...), password: str = Query(...)):
    try:
        print(f"Logging in user: {username}")
        response = cognito_client.initiate_auth(
            AuthFlow='USER_PASSWORD_AUTH',
            ClientId=CLIENT_ID,
            AuthParameters={
                'USERNAME': username,
                'PASSWORD': password,
            }
        )
        print(response)
        if 'ChallengeName' in response and response['ChallengeName'] == 'NEW_PASSWORD_REQUIRED':
            return {
                "access_token": None,
                "token_type": None,
                "challenge_name": "NEW_PASSWORD_REQUIRED",
                "session": response['Session']
            }
        
        access_token = response['AuthenticationResult']['AccessToken']
        return {"access_token": access_token, "token_type": "Bearer", "challenge_name": None, "session": None}

    except ClientError as e:
        print(e)
        raise HTTPException(status_code=400, detail=str(e))



# Endpoint to handle new password
@auth_router.post("/new-password")
async def set_new_password(
    username: str = Query(...),
    new_password: str = Query(...),
    session: str = Query(...)
):
    try:
        response = cognito_client.respond_to_auth_challenge(
            ClientId=CLIENT_ID,
            ChallengeName='NEW_PASSWORD_REQUIRED',
            Session=session,
            ChallengeResponses={
                'NEW_PASSWORD': new_password,
                'USERNAME': username,
            }
        )
        return {"message": "Password updated successfully."}
    except ClientError as e:
        raise HTTPException(status_code=400, detail=str(e))


# Token validation route
@auth_router.get("/auth/validate")
async def validate_token(token: str = Depends(oauth2_scheme)):
    try:
        response = cognito_client.get_user(
            AccessToken=token
        )
        username = response['Username']
        return {"authenticated": True, "user": username}

    except ClientError as e:
        raise HTTPException(status_code=401, detail="Token validation failed: " + str(e))


================================================
File: app/blog.py
================================================
import boto3
import json

from fastapi import APIRouter, Form, File, UploadFile, HTTPException, Query, Depends
from typing import Optional
from io import BytesIO
from math import ceil
import uuid
from concurrent.futures import ThreadPoolExecutor
from app.auth import validate_token
from app.utilities_S3 import download_object, get_s3_client, S3_BUCKET_NAME, S3_REGION

s3 = get_s3_client()
blog_router = APIRouter()

@blog_router.post("/add-blog", dependencies=[Depends(validate_token)])
async def create_post(
    id: str = Form(...),
    title: str = Form(...),
    subtitle: str = Form(...),
    author: str = Form(...),
    tags: str = Form(...),
    content: str = Form(...),
    draft: bool = Form(...),
    prev_image_url: Optional[str] = Form(None),
    image: Optional[UploadFile] = File(None),
):
    try:
        image_url = prev_image_url
        if image:
            print(f"Uploading image: {image.filename}")
            
            image_bytes = await image.read()
            random_filename = str(uuid.uuid4())
            s3_image_key = f"blog-images/{random_filename}"
            
            s3.upload_fileobj(
                BytesIO(image_bytes),
                S3_BUCKET_NAME,
                s3_image_key
            )
            
            image_url = f"https://{S3_BUCKET_NAME}.s3.{S3_REGION}.amazonaws.com/{s3_image_key}"
            print(f"Image uploaded successfully: {image_url}")
        
        blog_data = {
            "id": id,
            "title": title,
            "subtitle": subtitle,
            "author": author,
            "tags": tags.split(","),
            "content": content,
            "draft": draft,
            "image_url": image_url,
        }

        s3_json_key = f"blogs/{id}.json"
        json_data = json.dumps(blog_data, indent=4)

        s3.put_object(
            Bucket=S3_BUCKET_NAME,
            Key=s3_json_key,
            Body=json_data,
            ContentType='application/json'
        )
        
        download_object(s3_json_key, True)

        return {"message": "Post created successfully", "post": blog_data}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create post: {str(e)}")




@blog_router.get("/blogs")
async def list_posts(page: int = Query(1, ge=1), page_size: int = Query(10, ge=1, le=50), includeDrafts: bool = Query(False)):
    print("Listing posts")
    try:
        # Use paginator for listing objects in chunks
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=S3_BUCKET_NAME, Prefix="blogs/")

        blog_files = []

        # Collect all the blog files from all pages (just the keys, no download yet)
        for page_content in pages:
            if 'Contents' in page_content:
                blog_files.extend([file for file in page_content['Contents'] if file['Key'].endswith('.json')])

        if not blog_files:
            return {"message": "No posts found", "total_posts": 0, "total_pages": 0, "posts": []}

        # Calculate total posts and total pages
        total_posts = len(blog_files)
        total_pages = ceil(total_posts / page_size)

        # Handle out of range pages
        if page > total_pages:
            raise HTTPException(status_code=404, detail="Page not found")

        # Calculate start and end index for the current page
        start_index = (page - 1) * page_size
        end_index = min(start_index + page_size, total_posts)

        # Select the blog files that correspond to the requested page
        blog_files_for_page = blog_files[start_index:end_index]

        posts = []

        

        # Use ThreadPoolExecutor to download posts concurrently
        with ThreadPoolExecutor() as executor:
            results = list(executor.map(download_object, [file['Key'] for file in blog_files_for_page]))

        # Apply draft filter based on the includeDrafts parameter
        for post_data in results:
            if not includeDrafts and post_data.get('draft', False):
                continue
            posts.append(post_data)

        # Calculate the actual number of posts after filtering (if applicable)
        total_posts_after_filter = len(posts)
        
        print("Posts retrieved successfully")

        return {
            "message": "Posts retrieved successfully",
            "total_posts": total_posts_after_filter,
            "total_pages": total_pages,
            "current_page": page,
            "page_size": page_size,
            "posts": posts,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve posts: {str(e)}")


# Endpoint to get Blog details by ID
@blog_router.get("/blog/{blog_id}")
async def get_blog(blog_id: str):
    try:
        # Construct the S3 key using the Blog ID
        s3_json_key = f"blogs/{blog_id}.json"

        blog_data = download_object(s3_json_key)

        return {"message": "Blog retrieved successfully", "blog": blog_data}

    except s3.exceptions.NoSuchKey:
        raise HTTPException(status_code=404, detail="Blog not found")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve Blog: {str(e)}")

# Endpoint to delete a Blog by ID
@blog_router.delete("/blog/{blog_id}", dependencies=[Depends(validate_token)])
async def delete_blog(blog_id: str):
    try:
        # Construct the S3 key using the Blog ID
        s3_json_key = f"blogs/{blog_id}.json"

        # Check if the file exists before attempting to delete it
        try:
            s3.head_object(Bucket=S3_BUCKET_NAME, Key=s3_json_key)
        except s3.exceptions.NoSuchKey:
            raise HTTPException(status_code=404, detail="Blog not found")

        # Delete the Blog from S3
        s3.delete_object(Bucket=S3_BUCKET_NAME, Key=s3_json_key)

        return {"message": f"Blog with ID {blog_id} deleted successfully"}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete Blog: {str(e)}")


================================================
File: app/email.py
================================================
from fastapi import APIRouter, HTTPException, Form, Depends, Request
from fastapi.responses import JSONResponse
from slowapi import Limiter
from slowapi.util import get_remote_address
from pydantic import EmailStr
from boto3 import client, exceptions

# Initialize the SES client and Limiter
ses_client = client("ses", region_name="us-east-1") 
limiter = Limiter(key_func=get_remote_address)

contact_router = APIRouter()

# Rate limit: Max 5 requests per minute per IP
@contact_router.post("/contact-us")
@limiter.limit("5/minute")
async def contact_us(
    request: Request,
    name: str = Form(...),
    email: EmailStr = Form(...),
    message: str = Form(...),
):
    try:
        # Validate input length to avoid spamming
        if len(message) > 1000:
            raise HTTPException(status_code=400, detail="Message is too long")

        # Send email via AWS SES
        response = ses_client.send_email(
            Source="gennadiyshnayderman@gmail.com",  # Verified SES email
            Destination={
                "ToAddresses": ["gennadiyshnayderman@gmail.com"]  # Admin or support email
            },
            Message={
                "Subject": {"Data": f"New Contact Us Message from {name}"},
                "Body": {
                    "Text": {
                        "Data": f"Name: {name}\nEmail: {email}\nMessage:\n{message}"
                    }
                },
            },
        )
        return JSONResponse(
            status_code=200,
            content={"message": "Email sent successfully", "message_id": response["MessageId"]},
        )

    except exceptions.Boto3Error as e:
        raise HTTPException(status_code=500, detail=f"Failed to send email: {str(e)}")

    except HTTPException as e:
        raise e

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {str(e)}")



================================================
File: app/faq.py
================================================
import boto3
import json
from concurrent.futures import ThreadPoolExecutor
from fastapi import APIRouter, Form, HTTPException, Query, Depends
from math import ceil
from datetime import datetime
from app.auth import validate_token
from app.utilities_S3 import download_object, get_s3_client, S3_BUCKET_NAME, S3_REGION


faq_router = APIRouter()
s3 = get_s3_client()

# Endpoint to create an FAQ
@faq_router.post("/add-faq", dependencies=[Depends(validate_token)])
async def create_faq(
    id: str = Form(...),
    title: str = Form(...),
    answer: str = Form(...),
    draft: bool = Form(...),
):
    try:
        created_at = datetime.utcnow().isoformat()
        faq_data = {
            "id": id,
            "title": title,
            "answer": answer,
            "draft": draft,
            "created_at": created_at,
        }

        s3_json_key = f"faqs/{id}.json"
        json_data = json.dumps(faq_data, indent=4)

        s3.put_object(
            Bucket=S3_BUCKET_NAME,
            Key=s3_json_key,
            Body=json_data,
            ContentType='application/json'
        )
        download_object(s3_json_key, True)	
        return {"message": "FAQ created successfully", "faq": faq_data}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create FAQ: {str(e)}")

# Endpoint to list FAQs
@faq_router.get("/faqs")
async def list_faqs(page: int = Query(1, ge=1), page_size: int = Query(10, ge=1, le=50), includeDrafts: bool = Query(False)):
    try:
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=S3_BUCKET_NAME, Prefix="faqs/")

        faq_files = []
        for page_content in pages:
            if 'Contents' in page_content:
                faq_files.extend([file for file in page_content['Contents'] if file['Key'].endswith('.json')])

        if not faq_files:
            return {"message": "No FAQs found", "total_faqs": 0, "total_pages": 0, "faqs": []}

        total_faqs = len(faq_files)
        total_pages = ceil(total_faqs / page_size)

        if page > total_pages:
            raise HTTPException(status_code=404, detail="Page not found")

        start_index = (page - 1) * page_size
        end_index = min(start_index + page_size, total_faqs)
        faq_files_for_page = faq_files[start_index:end_index]

        faqs = []
        with ThreadPoolExecutor() as executor:
            results = list(executor.map(download_object, [file['Key'] for file in faq_files_for_page]))

        for faq_data in results:
            if not includeDrafts and faq_data.get('isDraft', False):
                continue
            faqs.append(faq_data)

        total_faqs_after_filter = len(faqs)

        return {
            "message": "FAQs retrieved successfully",
            "total_faqs": total_faqs_after_filter,
            "total_pages": total_pages,
            "current_page": page,
            "page_size": page_size,
            "faqs": faqs,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve FAQs: {str(e)}")

# Endpoint to get FAQ details by ID
@faq_router.get("/faq/{faq_id}")
async def get_faq(faq_id: str):
    try:
        s3_json_key = f"faqs/{faq_id}.json"
        faq_data = download_object(s3_json_key)
        return {"message": "FAQ retrieved successfully", "faq": faq_data}
    except s3.exceptions.NoSuchKey:
        raise HTTPException(status_code=404, detail="FAQ not found")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve FAQ: {str(e)}")

# Endpoint to delete an FAQ by ID
@faq_router.delete("/faq/{faq_id}", dependencies=[Depends(validate_token)])
async def delete_faq(faq_id: str):
    try:
        s3_json_key = f"faqs/{faq_id}.json"
        try:
            s3.head_object(Bucket=S3_BUCKET_NAME, Key=s3_json_key)
        except s3.exceptions.NoSuchKey:
            raise HTTPException(status_code=404, detail="FAQ not found")

        s3.delete_object(Bucket=S3_BUCKET_NAME, Key=s3_json_key)

        return {"message": f"FAQ with ID {faq_id} deleted successfully"}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete FAQ: {str(e)}")


================================================
File: app/gpt.py
================================================
import os
import json
import boto3
from botocore.exceptions import NoCredentialsError
from fastapi import  APIRouter, HTTPException, File, UploadFile, Form
from fastapi.responses import JSONResponse
from typing import List, Optional
import asyncio
from openai import OpenAI


gpt_router = APIRouter()

# Define a fixed prompt for validating questions
VALIDATION_PROMPT = """
This is a medical insurance blog application. Ensure that the following question and additional file content (if applicable) are relevant to medical insurance topics, such as health coverage, claims, policies, or any other health services related to insurance. If the question is not relevant, respond with an error message saying "Invalid question. Please ask a question related to medical insurance. Also answer greetings such as Hello with a polite message.".
"""

VALIDATION_PROMPT = ""

# Load your OpenAI API key from environment variables
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
)

# S3 configuration
S3_BUCKET_NAME = "medicare-blogs"
S3_REGION = "us-east-1"


# Initialize the S3 client
s3_client = boto3.client(
    "s3",
    region_name=S3_REGION,
)

@gpt_router.post("/ask-question")
async def ask_question(
    question: str = Form(...),
    previous_messages: Optional[str] = Form(None),
    files: Optional[List[UploadFile]] = File(None)
):
    content = [{"type": "text", "text": VALIDATION_PROMPT}]
    
    previous_message = ""
    if previous_messages:
        try:
            previous_messages = json.loads(previous_messages)
            for message in previous_messages:
                content.append({"type": "text", "text": message['text']})
                
        except json.JSONDecodeError:
            raise HTTPException(status_code=400, detail="Invalid format for previous messages.")
        
    content.append({"type": "text", "text": question})  
    # Process the file if it's uploaded

    if files and len(files) > 0:
        for file in files:
            s3_url = None
            try:
                if file.content_type.startswith("image/"):
                    # Upload the file to S3
                    s3_file_key = f"chat-images/{file.filename}"  # Define where to store the file
                    s3_client.upload_fileobj(file.file, S3_BUCKET_NAME, s3_file_key)
                    
                    # Generate a pre-signed URL for the uploaded file
                    s3_url = s3_client.generate_presigned_url(
                        'get_object',
                        Params={'Bucket': S3_BUCKET_NAME, 'Key': s3_file_key},
                        ExpiresIn=3600  # URL valid for 1 hour
                    )
                    print(f"Image uploaded successfully: {s3_url}")
                else:
                    raise HTTPException(status_code=400, detail="Only image files are allowed.")
            except NoCredentialsError:
                raise HTTPException(status_code=500, detail="AWS credentials not found.")
            except Exception as e:
                print(f"Error uploading to S3: {str(e)}")
                raise HTTPException(status_code=500, detail=f"Error uploading to S3: {str(e)}")
            
            if s3_url:
                content.append({
                            "type": "image_url",
                            "image_url": {
                                "url":s3_url }
                        })
      
    print(content)
    try:
        # Send the request to OpenAI API
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": content}],
        )
        
        response_text = response.choices[0].message.content
        
        print(response_text)

        # Check if the response indicates an invalid question
        if "Invalid question" in response_text:
            print("Invalid question detected.")
            return JSONResponse(content={
                "message": "Invalid question. Please ask a question related to medical insurance.",
                "question": question,
                "is_valid": False
            })

        return JSONResponse(content={
            "message": "Your question has been processed successfully.",
            "question": question,
            "response_from_openai": response_text,
            "previous_messages": previous_message,
            "is_valid": True
        })

    except asyncio.CancelledError:
        raise HTTPException(status_code=500, detail="Request was cancelled.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")


================================================
File: app/main.py
================================================
# main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.auth import auth_router
from app.blog import blog_router
from app.faq import faq_router
from app.gpt import gpt_router
from app.email import contact_router

app = FastAPI()

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://medicare-web-923948838.us-east-1.elb.amazonaws.com", "https://www.tantunai.com"],  # Allow only the frontend origin
    allow_credentials=True,
    allow_methods=["*"],  # Allow all HTTP methods (GET, POST, etc.)
    allow_headers=["*"],  # Allow all headers
)

# Include authentication routes
app.include_router(auth_router)
app.include_router(blog_router)
app.include_router(faq_router)
app.include_router(gpt_router)
app.include_router(contact_router)

================================================
File: app/utilities_S3.py
================================================
import boto3
import json
import redis
from fastapi import HTTPException

# AWS S3 Configuration
S3_BUCKET_NAME = "medicare-blogs"
S3_REGION = "us-east-1"
s3 = boto3.client(
    "s3",
    region_name=S3_REGION,
)

# Redis Configuration
redis_client = redis.StrictRedis(host='localhost', port=6379, db=0, decode_responses=True)  # Adjust host/port/db as needed

# Utility function to download an object from S3 or Redis
def download_object(file_key: str, skip_cache : bool = False, s3_bucket_name: str = S3_BUCKET_NAME) -> dict:
    try:
        if not skip_cache:
            # Check if the object is cached in Redis
            cached_data = redis_client.get(file_key)
            if cached_data:
                print(f"FAQ {file_key} found in Redis cache.")
                return json.loads(cached_data)

        # If not cached, fetch it from S3
        print(f"FAQ {file_key} not found in Redis. Fetching from S3.")
        s3_object = s3.get_object(Bucket=s3_bucket_name, Key=file_key)
        faq_data = json.loads(s3_object['Body'].read().decode('utf-8'))

        # Store the object in Redis for future requests (with an expiration time, e.g., 1 hour)
        redis_client.setex(file_key, 3600, json.dumps(faq_data))  # Cache for 3600 seconds (1 hour)

        return faq_data

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to download object: {str(e)}")


def get_s3_client():
    return s3


================================================
File: .github/workflows/aws.yml
================================================
# This workflow will build and push a new container image to Amazon ECR,
# and then will deploy a new task definition to Amazon ECS, when there is a push to the "master" branch.
#
# To use this workflow, you will need to complete the following set-up steps:
#
# 1. Create an ECR repository to store your images.
#    For example: `aws ecr create-repository --repository-name my-ecr-repo --region us-east-2`.
#    Replace the value of the `ECR_REPOSITORY` environment variable in the workflow below with your repository's name.
#    Replace the value of the `AWS_REGION` environment variable in the workflow below with your repository's region.
#
# 2. Create an ECS task definition, an ECS cluster, and an ECS service.
#    For example, follow the Getting Started guide on the ECS console:
#      https://us-east-2.console.aws.amazon.com/ecs/home?region=us-east-2#/firstRun
#    Replace the value of the `ECS_SERVICE` environment variable in the workflow below with the name you set for the Amazon ECS service.
#    Replace the value of the `ECS_CLUSTER` environment variable in the workflow below with the name you set for the cluster.
#
# 3. Store your ECS task definition as a JSON file in your repository.
#    The format should follow the output of `aws ecs register-task-definition --generate-cli-skeleton`.
#    Replace the value of the `ECS_TASK_DEFINITION` environment variable in the workflow below with the path to the JSON file.
#    Replace the value of the `CONTAINER_NAME` environment variable in the workflow below with the name of the container
#    in the `containerDefinitions` section of the task definition.
#
# 4. Store an IAM user access key in GitHub Actions secrets named `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
#    See the documentation for each action used below for the recommended IAM policies for this IAM user,
#    and best practices on handling the access key credentials.

name: Deploy to Amazon ECS

on:
  push:
    branches: [ "master" ]

env:
  AWS_REGION: "us-east-1"                   # set this to your preferred AWS region, e.g. us-west-1
  ECR_REPOSITORY: "medicare/backend"           # set this to your Amazon ECR repository name
  ECS_SERVICE: "medicare-api-with-lb"                 # set this to your Amazon ECS service name
  ECS_CLUSTER: "Medicare-cluster"                 # set this to your Amazon ECS cluster name
  ECS_TASK_DEFINITION: "./task-definition.json" # set this to the path to your Amazon ECS task definition
                                               # file, e.g. .aws/task-definition.json
  CONTAINER_NAME: "medicare-api"           # set this to the name of the container in the
                                               # containerDefinitions section of your task definition

permissions:
  contents: read

jobs:
  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    environment: production

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v1
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1

    - name: Build, tag, and push image to Amazon ECR
      id: build-image
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        # Build a docker container and
        # push it to ECR so that it can
        # be deployed to ECS.
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT

    - name: Fill in the new image ID in the Amazon ECS task definition
      id: task-def
      uses: aws-actions/amazon-ecs-render-task-definition@v1
      with:
        task-definition: ${{ env.ECS_TASK_DEFINITION }}
        container-name: ${{ env.CONTAINER_NAME }}
        image: ${{ steps.build-image.outputs.image }}
        environment-variables: |
            AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}
            AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}
            OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}

    - name: Deploy Amazon ECS task definition
      uses: aws-actions/amazon-ecs-deploy-task-definition@v1
      with:
        task-definition: ${{ steps.task-def.outputs.task-definition }}
        service: ${{ env.ECS_SERVICE }}
        cluster: ${{ env.ECS_CLUSTER }}
        wait-for-service-stability: true


